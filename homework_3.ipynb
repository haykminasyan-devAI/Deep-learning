{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check device \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13faed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "#prepare CIFAR-100 dataloaders with data augmentation\n",
    "def get_dataloaders(batch_size = 128):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding = 4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                             (0.2675, 0.2565, 0.2761))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                             (0.2675, 0.2565, 0.2761))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR100(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR100(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62211104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: VGG without BatchNorm\n",
    "\n",
    "class VGG_NoBN(nn.Module):\n",
    "    def __init__(self, num_classes = 100):\n",
    "        super(VGG_NoBN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            #Block 1\n",
    "            nn.Conv2d(3, 64, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(64, 64, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            #Block 2\n",
    "            nn.Conv2d(64, 128, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(128, 128, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            #Block3\n",
    "            nn.Conv2d(128, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 512),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal(m.weight, mode = 'fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model b: VGG with BatchNorm before activation\n",
    "class VGG_BN_Before(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(VGG_BN_Before, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d056b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model c: VGG with BatchNorm after activation\n",
    "class VGG_BN_After(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(VGG_BN_After, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function\n",
    "\n",
    "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(trainloader, desc = 'Training')\n",
    "    for inputs, targets in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        #Training steps\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Tracking metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Update progress bar with current loss and accuracy\n",
    "        pbar.set_postfix({'loss': running_loss/len(pbar), \n",
    "                         'acc': 100.*correct/total})\n",
    "    # Return average loss and accuracy for the epoch\n",
    "    return running_loss / len(trainloader), 100. * correct / total\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ace88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function with top-1 and top-5 accuracy\n",
    "def test(model, testloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(testloader, desc='Testing')\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Top-1 accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct_top1 += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Top-5 accuracy\n",
    "            _, top5_pred = outputs.topk(5, 1, True, True)\n",
    "            top5_pred = top5_pred.t()\n",
    "            correct_top5 += top5_pred.eq(targets.view(1, -1).expand_as(top5_pred)).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': running_loss/len(pbar),\n",
    "                            'top1': 100.*correct_top1/total,\n",
    "                            'top5': 100.*correct_top5/total})\n",
    "    \n",
    "    top1_acc = 100. * correct_top1 / total\n",
    "    top5_acc = 100. * correct_top5 / total\n",
    "    avg_loss = running_loss / len(testloader)\n",
    "    \n",
    "    return avg_loss, top1_acc, top5_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f44bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function\n",
    "def train_model(model, model_name, trainloader, testloader, num_epochs=30):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 40], gamma=0.1)\n",
    "    \n",
    "    best_top1_acc = 0.0\n",
    "    best_top5_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
    "        test_loss, top1_acc, top5_acc = test(model, testloader, criterion, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if top1_acc > best_top1_acc:\n",
    "            best_top1_acc = top1_acc\n",
    "            best_top5_acc = top5_acc\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Top-1 Acc: {top1_acc:.2f}%, Top-5 Acc: {top5_acc:.2f}%\")\n",
    "        print(f\"Best Top-1: {best_top1_acc:.2f}%, Best Top-5: {best_top5_acc:.2f}%\")\n",
    "    \n",
    "    return best_top1_acc, best_top5_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02394498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare data\n",
    "    trainloader, testloader = get_dataloaders(batch_size=128)\n",
    "    \n",
    "    # Store results\n",
    "    results = {}\n",
    "    \n",
    "    # Train model a: VGG without BatchNorm\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL A: VGG WITHOUT BATCH NORMALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    model_a = VGG_NoBN()\n",
    "    top1_a, top5_a = train_model(model_a, \"VGG without BatchNorm\", \n",
    "                                  trainloader, testloader, num_epochs=30)\n",
    "    results['No BatchNorm'] = {'top1': top1_a, 'top5': top5_a}\n",
    "    \n",
    "    # Train model b: VGG with BatchNorm before activation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL B: VGG WITH BATCH NORMALIZATION BEFORE ACTIVATION\")\n",
    "    print(\"=\"*60)\n",
    "    model_b = VGG_BN_Before()\n",
    "    top1_b, top5_b = train_model(model_b, \"VGG with BatchNorm before ReLU\", \n",
    "                                  trainloader, testloader, num_epochs=30)\n",
    "    results['BatchNorm Before ReLU'] = {'top1': top1_b, 'top5': top5_b}\n",
    "    \n",
    "    # Train model c: VGG with BatchNorm after activation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL C: VGG WITH BATCH NORMALIZATION AFTER ACTIVATION\")\n",
    "    print(\"=\"*60)\n",
    "    model_c = VGG_BN_After()\n",
    "    top1_c, top5_c = train_model(model_c, \"VGG with BatchNorm after ReLU\", \n",
    "                                  trainloader, testloader, num_epochs=30)\n",
    "    results['BatchNorm After ReLU'] = {'top1': top1_c, 'top5': top5_c}\n",
    "    \n",
    "    # Print final comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Model':<30} {'Top-1 Accuracy':<20} {'Top-5 Accuracy':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "    for model_name, acc in results.items():\n",
    "        print(f\"{model_name:<30} {acc['top1']:>18.2f}% {acc['top5']:>18.2f}%\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d5af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
